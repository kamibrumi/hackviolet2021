{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', style='whitegrid', font_scale=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = 'data/'\n",
    "#x_tr_M = np.loadtxt(os.path.join(DATA_DIR, 'SVI2018_US_COUNTY.csv'), delimiter=',', skiprows=1)\n",
    "\n",
    "data_svi2018_us_per_county = pd.read_csv(\"data/SVI2018_US_COUNTY.csv\") \n",
    "data_svi2018_us_per_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covid_cases_deaths_county = pd.read_csv(\"data/covid_cases_deaths_county.csv\", dtype={\"fips\": str}, parse_dates=['date']) \n",
    "data_covid_cases_deaths_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(np.asarray([data_covid_cases_deaths_county['deaths']>10000.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df = data_covid_cases_deaths_county[data_covid_cases_deaths_county['date'] == '2021-01-20']\n",
    "  \n",
    "rslt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covid_cases_deaths_county['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-owner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-reporter",
   "metadata": {},
   "source": [
    "# GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "postal-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aparently there are nans in this list but I cannot find them using numpy.isnan(myarray).any() so I remove them by doing:\n",
    "arr2 = np.asarray([int(x) for x in arr if str(x) != 'nan'])\n",
    "arr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_day = max(data_covid_cases_deaths_county['date']) # the earliest day in the dataset\n",
    "fst_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_fst_day_df = data_covid_cases_deaths_county[data_covid_cases_deaths_county['date'] == fst_day]\n",
    "rslt_fst_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_fips_fst_day = np.asarray([fips for fips in rslt_fst_day_df['fips'].unique()])\n",
    "print(unique_fips_fst_day.shape)\n",
    "unique_fips_fst_day= np.asarray([x for x in unique_fips_fst_day if str(x) != 'nan']) # converting to int and removing nans\n",
    "print(unique_fips_fst_day.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_fips_fst_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding a column with integer identificator starting from 0\n",
    "ids_nodes = np.arange(unique_fips_fst_day.shape[0])\n",
    "ids_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "death_per_county = list()\n",
    "for _id in unique_fips_fst_day:\n",
    "    n_deaths = rslt_fst_day_df.loc[rslt_fst_day_df['fips'] == _id, 'deaths']\n",
    "    if (n_deaths.shape[0] == 0):\n",
    "        death_per_county.append(0.0)\n",
    "    else:\n",
    "        death_per_county.append(n_deaths.to_numpy()[0])\n",
    "            \n",
    "\n",
    "death_per_county = np.asarray(death_per_county, dtype = np.float64)\n",
    "death_per_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_fst_day_df.loc[rslt_fst_day_df['fips'] == '1001', 'deaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change fips to be ints instead of strings so we can better manage the edges later\n",
    "unique_fips_fst_day = unique_fips_fst_day.astype(np.int64)\n",
    "unique_fips_fst_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can put together id, fips and deaths in a dataframe\n",
    "stack = np.hstack([ids_nodes.reshape((ids_nodes.shape[0],1)), unique_fips_fst_day.reshape((ids_nodes.shape[0],1)), death_per_county.reshape((ids_nodes.shape[0],1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_nodes.reshape((ids_nodes.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.DataFrame(data=stack, columns=[\"id\", \"fips\", \"deaths\"])\n",
    "print(nodes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the above is the data we are going to use for our nodes (1st layer).\n",
    "# now let's construct the edges data. We will take it from the counties adjacency data\n",
    "\n",
    "county_adjacency_df = pd.read_csv(\"data/county_adjacency2010.csv\", dtype={\"fips\": str}) \n",
    "county_adjacency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_adjacency_only_fips = county_adjacency_df[[\"fipscounty\", \"fipsneighbor\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the edges that are linking two different loops (we're avoiding loops)\n",
    "county_adjacency_only_fips = county_adjacency_only_fips[county_adjacency_only_fips['fipscounty'] != county_adjacency_only_fips['fipsneighbor']]\n",
    "county_adjacency_only_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "fipscouty = county_adjacency_only_fips['fipscounty'].to_numpy()\n",
    "fipscouty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fipsneighbor = county_adjacency_only_fips['fipsneighbor'].to_numpy()\n",
    "fipsneighbor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# associating the ids to the source and destination nodes that we assigned when creating the nodes dataframe\n",
    "ids_src_nodes = list()\n",
    "ids_dst_nodes = list()\n",
    "for i in range(fipscouty.shape[0]):\n",
    "    id_to_append_src = nodes_df.loc[nodes_df['fips'] == fipscouty[i], 'id'].to_numpy()\n",
    "    id_to_append_dst = nodes_df.loc[nodes_df['fips'] == fipsneighbor[i], 'id'].to_numpy()\n",
    "    \n",
    "    if (id_to_append_src.shape[0] != 0 and id_to_append_dst.shape[0] != 0):\n",
    "        ids_src_nodes.append(id_to_append_src[0])\n",
    "        ids_dst_nodes.append(id_to_append_dst[0])\n",
    "    \n",
    "print(ids_src_nodes)\n",
    "print('--')\n",
    "print(ids_dst_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_src_nodes = np.asarray(ids_src_nodes, dtype=np.int64)\n",
    "ids_dst_nodes = np.asarray(ids_dst_nodes, dtype=np.int64)\n",
    "\n",
    "stack_edges = np.hstack([ids_src_nodes.reshape((ids_src_nodes.shape[0],1)), ids_dst_nodes.reshape((ids_dst_nodes.shape[0],1))])\n",
    "edges_df = pd.DataFrame(data=stack_edges, columns=[\"src\", \"dst\"])\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have fewer numbers since there were counties ids in the adjacency dataframe that didn't match any county in\n",
    "# the nodes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "\n",
    "# now let's create the dgl graph\n",
    "src = edges_df['src'].to_numpy()\n",
    "dst = edges_df['dst'].to_numpy()\n",
    "\n",
    "# Create a DGL graph from a pair of numpy arrays\n",
    "g = dgl.graph((src, dst))\n",
    "\n",
    "# Print a graph gives some meta information such as number of nodes and edges.\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# Since the actual graph is undirected, we convert it for visualization\n",
    "# purpose.\n",
    "nx_g = g.to_networkx().to_undirected()\n",
    "# Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
    "pos = nx.kamada_kawai_layout(nx_g)\n",
    "nx.draw(nx_g, pos, with_labels=True, node_color=[[.7, .7, .7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df['deaths'].to_numpy() # to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I will load the node features, that is, the number of deaths.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Prepare the age node feature\n",
    "max_n_deaths = np.nanmax(nodes_df['deaths'].to_numpy())\n",
    "deaths = torch.tensor(nodes_df['deaths'].to_numpy()).float() / max_n_deaths\n",
    "print(deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the features to graph\n",
    "g.ndata['deaths'] = deaths # we are setting one of the keys of the ndata dictionary to be age with value age, which we created above.\n",
    "print(g) # in the output, the shape=() represents the feature shape. The features in this case is the float that represents the age of each one of the members of the club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a copy of the nodes df and add multiple layers of temporal data.\n",
    "# don't take the first day when covid appeared, but sometime when \n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "professional-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3206,)\n",
      "(3205,)\n",
      "[['0' '01001' '24.0']\n",
      " ['1' '01003' '47.0']\n",
      " ['2' '01005' '7.0']\n",
      " ...\n",
      " ['3202' '56041' '2.0']\n",
      " ['3203' '56043' '6.0']\n",
      " ['3204' '56045' '0.0']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fips</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>01001</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>01003</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>01005</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>01007</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>01009</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>3200</td>\n",
       "      <td>56037</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>3201</td>\n",
       "      <td>56039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3202</th>\n",
       "      <td>3202</td>\n",
       "      <td>56041</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>3203</td>\n",
       "      <td>56043</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>3204</td>\n",
       "      <td>56045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3205 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   fips  deaths\n",
       "0        0  01001    24.0\n",
       "1        1  01003    47.0\n",
       "2        2  01005     7.0\n",
       "3        3  01007     9.0\n",
       "4        4  01009    13.0\n",
       "...    ...    ...     ...\n",
       "3200  3200  56037     2.0\n",
       "3201  3201  56039     1.0\n",
       "3202  3202  56041     2.0\n",
       "3203  3203  56043     6.0\n",
       "3204  3204  56045     0.0\n",
       "\n",
       "[3205 rows x 3 columns]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we're gonna create the nodes from day Sept 14 and then add on top of that dataframe\n",
    "#def create_df_nodes_day(day_date, start_id):\n",
    "first_day_df = pd.to_datetime(['2020-09-14'])\n",
    "first_day = np.array(first_day_df, dtype= np.datetime64)[0]\n",
    "\n",
    "first_day_df = data_covid_cases_deaths_county[data_covid_cases_deaths_county['date'] == first_day]\n",
    "\n",
    "#removing duplicates\n",
    "unique_fips_first_day = np.asarray([fips for fips in first_day_df['fips'].unique()])\n",
    "print(unique_fips_first_day.shape)\n",
    "\n",
    "# removing nans\n",
    "unique_fips_first_day= np.asarray([x for x in unique_fips_first_day if str(x) != 'nan']) \n",
    "print(unique_fips_first_day.shape)\n",
    "\n",
    "# now adding a column with integer identificator starting from 0\n",
    "ids_nodes = np.arange(unique_fips_first_day.shape[0])\n",
    "\n",
    "#computing and crafting the number of deaths per county\n",
    "death_per_county = list()\n",
    "for _fips in unique_fips_first_day:\n",
    "    n_deaths = first_day_df.loc[first_day_df['fips'] == _fips, 'deaths']\n",
    "    if (n_deaths.shape[0] == 0):\n",
    "        death_per_county.append(0.0)\n",
    "    else:\n",
    "        death_per_county.append(n_deaths.to_numpy()[0])\n",
    "            \n",
    "\n",
    "death_per_county = np.asarray(death_per_county, dtype = np.float64)\n",
    "\n",
    "# change fips to be ints instead of strings so we can better manage the edges later\n",
    "#unique_fips_first_day = unique_fips_first_day.astype(np.int64)\n",
    "#print(unique_fips_first_day)\n",
    "# now we can put together id, fips and deaths in a dataframe\n",
    "stack_14 = np.hstack([ids_nodes.reshape((ids_nodes.shape[0],1)), unique_fips_first_day.reshape((ids_nodes.shape[0],1)), death_per_county.reshape((ids_nodes.shape[0],1))])\n",
    "print(stack_14)\n",
    "dict_14 = {\"id\": ids_nodes,\n",
    "          \"fips\": unique_fips_first_day,\n",
    "          \"deaths\": death_per_county}\n",
    "\n",
    "#nodes_df = pd.DataFrame(data=stack_14, columns=[\"id\", \"fips\", \"deaths\"], dtype={'fips': 'str'})\n",
    "nodes_df = pd.DataFrame(dict_14)\n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the edge dataframe\n",
    "\n",
    "county_adjacency_only_fips = county_adjacency_df[[\"fipscounty\", \"fipsneighbor\"]]\n",
    "\n",
    "# select only the edges that are linking two different nodes, we're not allowing loops (we're avoiding loops)\n",
    "county_adjacency_only_fips = county_adjacency_only_fips[county_adjacency_only_fips['fipscounty'] != county_adjacency_only_fips['fipsneighbor']]\n",
    "\n",
    "fipscouty = county_adjacency_only_fips['fipscounty'].to_numpy()\n",
    "fipsneighbor = county_adjacency_only_fips['fipsneighbor'].to_numpy()\n",
    "\n",
    "# associating the ids to the source and destination nodes that we assigned when creating the nodes dataframe\n",
    "ids_src_nodes = list()\n",
    "ids_dst_nodes = list()\n",
    "for i in range(fipscouty.shape[0]):\n",
    "    id_to_append_src = nodes_df.loc[nodes_df['fips'] == fipscouty[i], 'id'].to_numpy()\n",
    "    id_to_append_dst = nodes_df.loc[nodes_df['fips'] == fipsneighbor[i], 'id'].to_numpy()\n",
    "    \n",
    "    if (id_to_append_src.shape[0] != 0 and id_to_append_dst.shape[0] != 0):\n",
    "        ids_src_nodes.append(id_to_append_src[0])\n",
    "        ids_dst_nodes.append(id_to_append_dst[0])\n",
    "    \n",
    "ids_src_nodes = np.asarray(ids_src_nodes, dtype=np.int64)\n",
    "ids_dst_nodes = np.asarray(ids_dst_nodes, dtype=np.int64)\n",
    "\n",
    "stack_edges = np.hstack([ids_src_nodes.reshape((ids_src_nodes.shape[0],1)), ids_dst_nodes.reshape((ids_dst_nodes.shape[0],1))])\n",
    "edges_df = pd.DataFrame(data=stack_edges, columns=[\"src\", \"dst\"])\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "atlantic-domestic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18756"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = edges_df.index\n",
    "number_of_rows = len(index)\n",
    "number_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll create a function that it will make it easier to add new layers\n",
    "# day_date is already a date object in python\n",
    "# start id is the id that the last node in the last layer has\n",
    "def create_df_nodes_day(day_date, start_id, n_nodes):\n",
    "    # we're gonna create the nodes from day Sept 14 and then add on top of that dataframe\n",
    "    # copy the node_df as a reference of how the fips are organized\n",
    "    node_df_copy = nodes_df.copy(deep=True)\n",
    "    \n",
    "    # from the following df we will get the number of deaths\n",
    "    other_day_df = data_covid_cases_deaths_county[data_covid_cases_deaths_county['date'] == day_date]\n",
    "\n",
    "    print(node_df_copy)\n",
    "    print(other_day_df)\n",
    "    # now adding a column with integer identificator starting from 0\n",
    "    ids_nodes = np.arange(start_id, start_id + n_nodes)\n",
    "    \n",
    "    # now we can get rid of the previous ids column and add the new one\n",
    "    node_df_copy.drop(['id'], axis=1)\n",
    "    node_df_copy['id'] = ids_nodes\n",
    "\n",
    "    # changing type of fips column in other_day_df in order to be able to do other_day_df['fips'] == fips_id below\n",
    "    #other_day_df.astype({'fips': 'int64'}, errors='ignore').dtypes\n",
    "    \n",
    "    #computing and crafting the number of deaths per county\n",
    "    death_per_county = list()\n",
    "    for i in node_df_copy.itertuples(): \n",
    "        fips_id = i[2]\n",
    "        #print(fips_id, i)\n",
    "        n_deaths = other_day_df.loc[other_day_df['fips'] == fips_id, 'deaths']\n",
    "        print(n_deaths)\n",
    "        if (n_deaths.shape[0] == 0):\n",
    "            death_per_county.append(0.0)\n",
    "        else:\n",
    "            death_per_county.append(n_deaths.to_numpy()[0])\n",
    "\n",
    "    death_per_county = np.asarray(death_per_county, dtype = np.float64)\n",
    "    \n",
    "    # now we can get rid of the previous deaths column in the nodes_df and add the new deaths column\n",
    "    node_df_copy.drop(['deaths'], axis=1)\n",
    "    node_df_copy['deaths'] = death_per_county \n",
    "    return node_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#days_df =pd.to_datetime(['2020-09-15', '2020-09-16', '2020-09-17', '2020-09-19'])\n",
    "days_df =pd.to_datetime(['2020-09-29'])\n",
    "days_np_arr = np.array(days_df, dtype= np.datetime64)\n",
    "\n",
    "index = nodes_df.index # instead of node_df, always remember to put the last node dataframe created\n",
    "start_id = len(index)\n",
    "n_nodes = len(index) # in this case start_id and n_nodes are the same but it will not be like this after the second layer\n",
    "\n",
    "node_df_day_15 = create_df_nodes_day(days_np_arr[0], start_id, n_nodes)\n",
    "node_df_day_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "collected-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date      county    state   fips  cases  deaths\n",
      "534478 2020-09-15     Autauga  Alabama  01001   1601    24.0\n",
      "534479 2020-09-15     Baldwin  Alabama  01003   4992    47.0\n",
      "534480 2020-09-15     Barbour  Alabama  01005    806     7.0\n",
      "534481 2020-09-15        Bibb  Alabama  01007    611     9.0\n",
      "534482 2020-09-15      Blount  Alabama  01009   1475    13.0\n",
      "...           ...         ...      ...    ...    ...     ...\n",
      "537709 2020-09-15  Sweetwater  Wyoming  56037    317     2.0\n",
      "537710 2020-09-15       Teton  Wyoming  56039    478     1.0\n",
      "537711 2020-09-15       Uinta  Wyoming  56041    312     2.0\n",
      "537712 2020-09-15    Washakie  Wyoming  56043    110     6.0\n",
      "537713 2020-09-15      Weston  Wyoming  56045     23     0.0\n",
      "\n",
      "[3236 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "for day in days_np_arr:\n",
    "    rslt_day_i_df = data_covid_cases_deaths_county[data_covid_cases_deaths_county['date'] == day]\n",
    "    print(rslt_day_i_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more ideas:\n",
    "# add also number of cases! not only deaths\n",
    "# in order to give the doctors intuition on where they should send personnel\n",
    "# and resources to treat the patients.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
